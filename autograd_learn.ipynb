{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算图是现代深度学习框架的核心\n",
    "#手动求取反向传播废时费力，而且容易出错\n",
    "#autograd.Variable封装了data,grad,grad_fn三个数据\n",
    "#data是tensor，grad的形状与data一致，grad_fn指向一个Func记录操作历史\n",
    "\n",
    "#可选参数\n",
    "#requires_grad(bool):是否对该ariable求导数\n",
    "#volatile(bool):设置为True，构建在该variable上的图都不会求道，为推理阶段设计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable as var\n",
    "from torch.autograd import Function\n",
    "\n",
    "#从tensor中创建var，指定需要求导\n",
    "a = var(t.ones(3,4),requires_grad = True)\n",
    "b = var(t.zeros(3,4))\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = a+b\n",
    "print(c)\n",
    "c = a.add(b)\n",
    "print(c)\n",
    "#上面两种方法一样结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.sum()\n",
    "d.backward()#反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#注意区别，第一种因为先取了data部分再sum，所以数据类型是tensor\n",
    "c.data.sum()\n",
    "#第二种因为直接sum,所以数据类型仍然为var\n",
    "c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#例子求y=x^2 * e^x的导数\n",
    "x = var(t.randn(3,4),requires_grad=True)\n",
    "\n",
    "def f(x):\n",
    "    '''定义y'''\n",
    "    y = x**2 * t.exp(x)\n",
    "    return y\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "def gradf(x):\n",
    "    '''手动求导'''\n",
    "    dx = 2*x*t.exp(x) +x**2*t.exp(x)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动求导数:\n",
      "tensor([[-0.1885, -0.1643,  2.3835, 12.7910],\n",
      "        [20.3103, -0.4363, -0.3132, -0.2113],\n",
      "        [ 1.2361, -0.2959, -0.3196,  6.1649]], grad_fn=<AddBackward0>)\n",
      "自动求导数：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1885, -0.1643,  2.3835, 12.7910],\n",
       "        [20.3103, -0.4363, -0.3132, -0.2113],\n",
       "        [ 1.2361, -0.2959, -0.3196,  6.1649]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"手动求导数:\")\n",
    "dx = gradf(x)\n",
    "print(dx)\n",
    "y.backward(t.ones(y.size()))#确保grad_var的形状与y一样\n",
    "print(\"自动求导数：\")  \n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = var(t.ones(1))\n",
    "b = var(t.rand(1),requires_grad = True)\n",
    "w = var(t.rand(1),requires_grad = True)\n",
    "y = w*x\n",
    "z = y+b\n",
    "#只要涉及到需要求道的就会自动变为求导\n",
    "x.requires_grad,b.requires_grad,w.requires_grad,z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x7f5638c2d310>, <MulBackward0 at 0x7f56395691d0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grad_fn可以查看这个var的反向传播函数，比如z的反向传播函数就是add backward\n",
    "#而y是乘法反向\n",
    "z.grad_fn,y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#像叶子节点，无法反向的，就会成为none\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
