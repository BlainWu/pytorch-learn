{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算图是现代深度学习框架的核心\n",
    "#手动求取反向传播废时费力，而且容易出错\n",
    "#autograd.Variable封装了data,grad,grad_fn三个数据\n",
    "#data是tensor，grad的形状与data一致，grad_fn指向一个Func记录操作历史\n",
    "\n",
    "#可选参数\n",
    "#requires_grad(bool):是否对该ariable求导数\n",
    "#volatile(bool):设置为True，构建在该variable上的图都不会求道，为推理阶段设计\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable as var\n",
    "\n",
    "#从tensor中创建var，指定需要求导\n",
    "a = var(t.ones(3,4),requires_grad = True)\n",
    "b = var(t.zeros(3,4))\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a+b\n",
    "print(c)\n",
    "c = a.add(b)\n",
    "print(c)\n",
    "#上面两种方法一样结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.sum()\n",
    "d.backward()#反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意区别，第一种因为先取了data部分再sum，所以数据类型是tensor\n",
    "c.data.sum()\n",
    "#第二种因为直接sum,所以数据类型仍然为var\n",
    "c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#例子求y=x^2 * e^x的导数\n",
    "x = var(t.randn(3,4),requires_grad=True)\n",
    "\n",
    "def f(x):\n",
    "    '''定义y'''\n",
    "    y = x**2 * t.exp(x)\n",
    "    return y\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "def gradf(x):\n",
    "    '''手动求导'''\n",
    "    dx = 2*x*t.exp(x) +x**2*t.exp(x)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动求导数:\n",
      "tensor([[ 2.4460e+00,  1.6331e+00, -4.0214e-01,  3.0908e+01],\n",
      "        [ 2.1939e+00,  5.3760e-01,  2.3291e-02,  2.4220e+01],\n",
      "        [-4.1649e-01, -4.3951e-01, -2.1646e-01,  5.1797e+00]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "自动求导数：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4460e+00,  1.6331e+00, -4.0214e-01,  3.0908e+01],\n",
       "        [ 2.1939e+00,  5.3760e-01,  2.3291e-02,  2.4220e+01],\n",
       "        [-4.1649e-01, -4.3951e-01, -2.1646e-01,  5.1797e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"手动求导数:\")\n",
    "dx = gradf(x)\n",
    "print(dx)\n",
    "y.backward(t.ones(y.size()))#确保grad_var的形状与y一样\n",
    "print(\"自动求导数：\")  \n",
    "x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
